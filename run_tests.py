"""
Comprehensive test runner for Red Hat Status Checker.
Tests all modules, flags, and integration scenarios.
"""
import os
import sys
import subprocess
import argparse
from pathlib import Path


def run_pytest_with_coverage():
    """Run pytest with coverage reporting"""
    project_root = Path(__file__).parent
    
    # Define test command with coverage
    cmd = [
        sys.executable, '-m', 'pytest',
        str(project_root / 'tests'),
        '-v',
        '--tb=short',
        '--strict-markers',
        '--disable-warnings',
        '--cov=redhat_status',
        '--cov-report=term-missing',
        '--cov-report=html:htmlcov',
        '--cov-report=xml:coverage.xml',
        '--junit-xml=junit.xml'
    ]
    
    print("Running comprehensive test suite with coverage...")
    print(f"Command: {' '.join(cmd)}")
    print("=" * 60)
    
    try:
        result = subprocess.run(cmd, cwd=str(project_root))
        return result.returncode
    except Exception as e:
        print(f"Error running tests: {e}")
        return 1


def run_specific_test_module(module_name):
    """Run tests for a specific module"""
    project_root = Path(__file__).parent
    test_file = project_root / 'tests' / f'test_{module_name}.py'
    
    if not test_file.exists():
        print(f"Test file not found: {test_file}")
        return 1
    
    cmd = [
        sys.executable, '-m', 'pytest',
        str(test_file),
        '-v',
        '--tb=short'
    ]
    
    print(f"Running tests for module: {module_name}")
    print(f"Command: {' '.join(cmd)}")
    print("=" * 60)
    
    try:
        result = subprocess.run(cmd, cwd=str(project_root))
        return result.returncode
    except Exception as e:
        print(f"Error running tests: {e}")
        return 1


def run_flag_tests():
    """Run comprehensive tests for all CLI flags"""
    project_root = Path(__file__).parent
    script_path = project_root / 'redhat_status.py'
    
    # Complete list of all flags to test - organized by category
    flags_to_test = [
        # Core System (2)
        ['--help'],
        ['--version'],
        
        # Operation Modes (5)
        ['quick'],
        ['simple'],
        ['full'],
        ['export'],
        ['all'],
        
        # Output Control (2)
        ['quick', '--quiet'],
        ['quick', '--output', '/tmp/test_output.txt'],
        
        # Performance (5)
        ['quick', '--performance'],
        ['quick', '--benchmark'],
        ['quick', '--concurrent-check'],
        ['--no-cache', 'quick'],
        ['--clear-cache'],
        
        # Configuration (2)
        ['--config-check'],
        ['--setup'],
        
        # Enterprise DB (1)
        ['--db-maintenance'],
        
        # Notifications (2)
        ['--test-notifications'],
        ['quick', '--notify'],
        
        # AI Analytics (7)
        ['--analytics-summary'],
        ['--ai-insights'],
        ['--anomaly-analysis'],
        ['--health-report'],
        ['--insights'],
        ['--trends'],
        ['--slo-dashboard'],
        
        # Export Features (3)
        ['--export-ai-report'],
        ['--export-history'],
        ['export', '--format', 'json'],
        ['export', '--format', 'csv'],
        ['export', '--format', 'txt'],
        
        # Service Operations (5)
        ['simple', '--filter', 'all'],
        ['simple', '--filter', 'issues'],
        ['simple', '--filter', 'operational'],
        ['simple', '--filter', 'degraded'],
        ['simple', '--search', 'test'],
        
        # Monitoring (2)
        ['--watch', '5'],  # Fixed: --watch requires SECONDS parameter
        ['--enable-monitoring'],
        
        # Debug (4)
        ['--log-level', 'DEBUG', 'quick'],
        ['--log-level', 'INFO', 'quick'],
        ['--log-level', 'WARNING', 'quick'],
        ['--log-level', 'ERROR', 'quick'],
    ]
    
    print("Testing all CLI flags...")
    print("=" * 80)
    
    passed = 0
    failed = 0
    categories = {
        "Core System": 2,
        "Operation Modes": 5, 
        "Output Control": 2,
        "Performance": 5,
        "Configuration": 2,
        "Enterprise DB": 1,
        "Notifications": 2,
        "AI Analytics": 7,
        "Export Features": 3,
        "Service Operations": 5,
        "Monitoring": 2,
        "Debug": 4
    }
    
    category_results = {}
    current_category = ""
    category_index = 0
    
    category_names = list(categories.keys())
    category_counts = list(categories.values())
    
    for i, flags in enumerate(flags_to_test):
        # Determine current category
        if category_index < len(category_counts):
            if i >= sum(category_counts[:category_index + 1]):
                category_index += 1
        
        if category_index < len(category_names):
            new_category = category_names[category_index]
            if new_category != current_category:
                current_category = new_category
                print(f"\nüìÇ {current_category}")
                print("-" * 40)
                category_results[current_category] = {"passed": 0, "failed": 0}
        
        cmd = [sys.executable, str(script_path)] + flags
        print(f"Testing: {' '.join(flags)}")
        
        try:
            # Special timeout for monitoring commands like --watch
            timeout = 10 if '--watch' in flags else 30
            
            # Run with timeout to prevent hanging
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=str(project_root)
            )
            
            # Special handling for different flag types
            if flags[0] in ['--help', '--version']:
                # Help and version commands exit with non-zero but are successful
                success = True
            elif flags[0] in ['--setup', '--enable-monitoring'] or '--watch' in flags:
                # These flags might have special exit behavior or timeout gracefully
                success = result.returncode in [0, 1, 130]  # Allow success, graceful exit, and SIGINT
            else:
                success = result.returncode == 0
            
            if success:
                print(f"  ‚úÖ PASSED")
                passed += 1
                if current_category in category_results:
                    category_results[current_category]["passed"] += 1
            else:
                print(f"  ‚ùå FAILED (exit code: {result.returncode})")
                if result.stderr:
                    print(f"     Error: {result.stderr[:100]}...")
                failed += 1
                if current_category in category_results:
                    category_results[current_category]["failed"] += 1
                
        except subprocess.TimeoutExpired:
            # For watch commands, timeout is expected and considered success
            if '--watch' in flags:
                print(f"  ‚úÖ PASSED (timeout expected for monitoring)")
                passed += 1
                if current_category in category_results:
                    category_results[current_category]["passed"] += 1
            else:
                print(f"  ‚è∞ TIMEOUT")
                failed += 1
                if current_category in category_results:
                    category_results[current_category]["failed"] += 1
        except Exception as e:
            print(f"  ‚ùå ERROR: {e}")
            failed += 1
            if current_category in category_results:
                category_results[current_category]["failed"] += 1
    
    # Print category summary
    print("\n" + "=" * 80)
    print("üìä CATEGORY SUMMARY")
    print("=" * 80)
    for category, results in category_results.items():
        total = results["passed"] + results["failed"]
        status = "‚úÖ PASS" if results["failed"] == 0 else "‚ùå FAIL"
        print(f"{category:20} {results['passed']:2}/{total:2} passed {status}")
    
    print("\n" + "=" * 80)
    print(f"üéØ OVERALL FLAG TESTS: {passed} passed, {failed} failed")
    total_expected = sum(categories.values())
    print(f"üìà COVERAGE: {passed}/{total_expected} flags tested ({passed/total_expected*100:.1f}%)")
    
    return 0 if failed == 0 else 1


def run_example_commands_test():
    """Test the specific example commands from documentation"""
    project_root = Path(__file__).parent
    script_path = project_root / 'redhat_status.py'
    main_path = project_root / 'redhat_status' / 'main.py'
    
    # Example commands to test (from user's documentation)
    example_commands = [
        # Main entry points
        (['python3', str(script_path), 'quick'], "Quick global status check"),
        (['python3', str(main_path), 'quick'], "Quick via main.py entry point"),
        (['python3', str(script_path), 'quick', '--quiet'], "Quiet mode for scripting"),
        (['python3', str(script_path), 'simple'], "Main services monitoring"),
        (['python3', str(script_path), 'full'], "Complete service hierarchy"),
        (['python3', str(script_path), 'export'], "Export data to files"),
        (['python3', str(script_path), 'all'], "Display everything"),
        
        # Additional advanced commands
        (['python3', str(script_path), 'quick', '--performance'], "Show performance metrics"),
        (['python3', str(script_path), '--config-check'], "Configuration validation"),
        (['python3', str(script_path), '--test-notifications'], "Test notification channels"),
        (['python3', str(script_path), '--analytics-summary'], "AI analytics summary"),
        (['python3', str(script_path), '--db-maintenance'], "Database maintenance"),
        (['python3', str(script_path), '--clear-cache'], "Clear cache"),
    ]
    
    # Test interactive mode separately (requires special handling)
    interactive_test = (['timeout', '3', 'python3', str(script_path)], "Interactive mode - choose operation")
    
    print("Testing example commands from documentation...")
    print("=" * 80)
    
    passed = 0
    failed = 0
    
    for cmd, description in example_commands:
        print(f"Testing: {' '.join(cmd[1:])}")  # Skip 'python3' in display
        print(f"Description: {description}")
        
        try:
            # Use sys.executable instead of 'python3' for compatibility
            test_cmd = [sys.executable] + cmd[1:]
            
            result = subprocess.run(
                test_cmd,
                capture_output=True,
                text=True,
                timeout=30,
                cwd=str(project_root)
            )
            
            # Check if command executed successfully
            success = result.returncode == 0
            
            if success:
                print(f"  ‚úÖ PASSED")
                passed += 1
            else:
                print(f"  ‚ùå FAILED (exit code: {result.returncode})")
                if result.stderr:
                    print(f"     Error: {result.stderr[:150]}...")
                failed += 1
                
        except subprocess.TimeoutExpired:
            print(f"  ‚è∞ TIMEOUT")
            failed += 1
        except Exception as e:
            print(f"  ‚ùå ERROR: {e}")
            failed += 1
        
        print()  # Add spacing between tests
    
    # Test interactive mode with timeout
    print(f"Testing: {' '.join(interactive_test[0][2:])}")  # Skip 'timeout' and '3' in display
    print(f"Description: {interactive_test[1]}")
    
    try:
        # Test interactive mode with timeout
        result = subprocess.run(
            interactive_test[0],
            capture_output=True,
            text=True,
            cwd=str(project_root)
        )
        
        # For interactive mode, timeout (exit code 124) or success (exit code 0) are both acceptable
        success = result.returncode in [0, 124]  # 124 is timeout exit code
        
        if success:
            print(f"  ‚úÖ PASSED (interactive mode started successfully)")
            passed += 1
        else:
            print(f"  ‚ùå FAILED (exit code: {result.returncode})")
            if result.stderr:
                print(f"     Error: {result.stderr[:150]}...")
            failed += 1
            
    except Exception as e:
        print(f"  ‚ùå ERROR: {e}")
        failed += 1
    
    print()
    
    print("=" * 80)
    print(f"üìä EXAMPLE COMMANDS TEST SUMMARY")
    print(f"üéØ Results: {passed} passed, {failed} failed")
    total_commands = len(example_commands) + 1  # +1 for interactive mode
    print(f"üìà Success Rate: {passed}/{total_commands} ({passed/total_commands*100:.1f}%)")
    
    return 0 if failed == 0 else 1


def run_advanced_commands_test():
    """Test advanced command combinations and specific use cases"""
    project_root = Path(__file__).parent
    script_path = project_root / 'redhat_status.py'
    
    # Advanced command combinations to test
    advanced_commands = [
        # AI Analytics Commands
        (['python3', str(script_path), '--ai-insights'], "Detailed AI analysis with confidence scores and patterns"),
        (['python3', str(script_path), '--anomaly-analysis'], "Advanced anomaly detection with severity levels"),
        (['python3', str(script_path), '--insights'], "System insights and behavioral patterns"),
        (['python3', str(script_path), '--trends'], "Availability trends and predictive analysis"),
        (['python3', str(script_path), '--slo-dashboard'], "SLO tracking and objectives dashboard"),
        (['python3', str(script_path), '--health-report'], "Comprehensive health analysis with grading (A+ to F)"),
        
        # Export AI Reports
        (['python3', str(script_path), '--export-ai-report', '--format', 'json'], "Export AI analysis to JSON format"),
        (['python3', str(script_path), '--export-ai-report', '--format', 'csv'], "Export AI analysis to CSV format"),
        
        # Filtering Commands
        (['python3', str(script_path), '--filter', 'issues'], "Show only services with issues"),
        (['python3', str(script_path), '--filter', 'operational'], "Show only operational services"),
        (['python3', str(script_path), '--filter', 'degraded'], "Show services with degraded performance"),
        
        # Search Commands
        (['python3', str(script_path), '--search', 'openshift'], "Search for OpenShift services"),
        (['python3', str(script_path), '--search', 'satellite'], "Search for Satellite services"),
        (['python3', str(script_path), '--search', 'registry', '--filter', 'issues'], "Combine filtering and search"),
        
        # Monitoring Commands (fixed timeout commands)
        (['python3', str(script_path), '--watch', '5'], "Live monitoring with 5-second refresh (short test)"),
        (['python3', str(script_path), '--watch', '10', '--quiet'], "Quiet live monitoring for dashboards"),
        (['python3', str(script_path), '--enable-monitoring'], "Enable continuous monitoring features"),
        
        # Notification Commands
        (['python3', str(script_path), '--notify'], "Send immediate status notifications"),
        (['python3', str(script_path), '--test-notifications'], "Test all notification channels"),
        
        # Performance Commands
        (['python3', str(script_path), '--benchmark'], "Run performance benchmarking tests"),
        (['python3', str(script_path), '--concurrent-check'], "Enable multi-threaded health checks"),
        (['python3', str(script_path), '--no-cache'], "Bypass cache for fresh data"),
        (['python3', str(script_path), 'quick', '--performance'], "Show detailed performance metrics"),
        
        # Export History Commands
        (['python3', str(script_path), '--export-history'], "Export historical data in JSON format"),
        (['python3', str(script_path), '--export-history', '--format', 'csv'], "Export historical data in CSV format"),
        (['python3', str(script_path), '--export-history', '--output', './reports'], "Export to specific directory"),
        
        # Logging Commands
        (['python3', str(script_path), '--log-level', 'DEBUG'], "Enable debug logging"),
        (['python3', str(script_path), '--log-level', 'WARNING'], "Warning level only"),
        (['python3', str(script_path), '--log-level', 'ERROR'], "Error level only"),
        
        # Configuration Commands
        (['python3', str(script_path), '--config-check'], "Validate configuration files"),
        (['python3', str(script_path), '--setup'], "Run configuration setup wizard"),
        (['python3', str(script_path), '--db-maintenance'], "Database maintenance and cleanup"),
    ]
    
    print("Testing advanced command combinations...")
    print("=" * 80)
    
    passed = 0
    failed = 0
    
    # Group commands by category for better reporting
    categories = {
        "AI Analytics": 6,
        "Export AI Reports": 2,
        "Filtering": 3,
        "Search": 3,
        "Monitoring": 3,
        "Notifications": 2,
        "Performance": 4,
        "Export History": 3,
        "Logging": 3,
        "Configuration": 3
    }
    
    category_results = {}
    current_category = ""
    category_index = 0
    category_names = list(categories.keys())
    category_counts = list(categories.values())
    
    for i, (cmd, description) in enumerate(advanced_commands):
        # Determine current category
        if category_index < len(category_counts):
            if i >= sum(category_counts[:category_index + 1]):
                category_index += 1
        
        if category_index < len(category_names):
            new_category = category_names[category_index]
            if new_category != current_category:
                current_category = new_category
                print(f"\nüìÇ {current_category}")
                print("-" * 40)
                category_results[current_category] = {"passed": 0, "failed": 0}
        
        print(f"Testing: {' '.join(cmd[1:])}")  # Skip 'python3' in display
        print(f"Description: {description}")
        
        try:
            # Use sys.executable instead of 'python3' for compatibility
            test_cmd = [sys.executable] + cmd[1:]
            
            # Special timeout for monitoring and concurrent commands
            if '--watch' in cmd:
                timeout = 8  # Short timeout for watch commands
            elif '--concurrent-check' in cmd:
                timeout = 45  # Longer timeout for concurrent operations
            else:
                timeout = 30
            
            result = subprocess.run(
                test_cmd,
                capture_output=True,
                text=True,
                timeout=timeout,
                cwd=str(project_root)
            )
            
            # Special handling for different command types
            if '--setup' in cmd or '--enable-monitoring' in cmd:
                # These commands might have special exit behavior
                success = result.returncode in [0, 1]
            else:
                success = result.returncode == 0
            
            if success:
                print(f"  ‚úÖ PASSED")
                passed += 1
                if current_category in category_results:
                    category_results[current_category]["passed"] += 1
            else:
                print(f"  ‚ùå FAILED (exit code: {result.returncode})")
                if result.stderr:
                    print(f"     Error: {result.stderr[:150]}...")
                failed += 1
                if current_category in category_results:
                    category_results[current_category]["failed"] += 1
                
        except subprocess.TimeoutExpired:
            if '--watch' in cmd or '--concurrent-check' in cmd:
                print(f"  ‚úÖ PASSED (timeout expected for monitoring/concurrent operations)")
                passed += 1
                if current_category in category_results:
                    category_results[current_category]["passed"] += 1
            else:
                print(f"  ‚è∞ TIMEOUT")
                failed += 1
                if current_category in category_results:
                    category_results[current_category]["failed"] += 1
        except Exception as e:
            print(f"  ‚ùå ERROR: {e}")
            failed += 1
            if current_category in category_results:
                category_results[current_category]["failed"] += 1
        
        print()  # Add spacing between tests
    
    # Print category summary
    print("=" * 80)
    print("üìä ADVANCED COMMANDS CATEGORY SUMMARY")
    print("=" * 80)
    for category, results in category_results.items():
        total = results["passed"] + results["failed"]
        status = "‚úÖ PASS" if results["failed"] == 0 else "‚ùå FAIL"
        print(f"{category:20} {results['passed']:2}/{total:2} passed {status}")
    
    print("\n" + "=" * 80)
    print(f"üéØ ADVANCED COMMANDS TEST RESULTS")
    print(f"Results: {passed} passed, {failed} failed")
    print(f"üìà Success Rate: {passed}/{len(advanced_commands)} ({passed/len(advanced_commands)*100:.1f}%)")
    
    return 0 if failed == 0 else 1


def install_test_dependencies():
    """Install test dependencies"""
    dependencies = [
        'pytest>=7.0.0',
        'pytest-cov>=4.0.0',
        'pytest-mock>=3.0.0',
        'pytest-asyncio>=0.21.0',
        'coverage>=6.0.0'
    ]
    
    print("Installing test dependencies...")
    for dep in dependencies:
        cmd = [sys.executable, '-m', 'pip', 'install', dep]
        try:
            subprocess.run(cmd, check=True, capture_output=True)
            print(f"  ‚úÖ Installed {dep}")
        except subprocess.CalledProcessError as e:
            print(f"  ‚ùå Failed to install {dep}: {e}")
            return 1
    
    return 0


def run_container_tests():
    """Run container-specific tests"""
    print("\n" + "="*60)
    print("üê≥ RUNNING CONTAINER TESTS")
    print("="*60)
    
    project_root = Path(__file__).parent
    
    try:
        # Check if container runtime is available
        runtime_available = False
        
        # Check for Podman
        try:
            subprocess.run(['podman', '--version'], 
                          capture_output=True, check=True, timeout=10)
            print("‚úÖ Podman detected")
            runtime_available = True
        except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
            pass
        
        # Check for Docker if Podman not available
        if not runtime_available:
            try:
                subprocess.run(['docker', '--version'], 
                              capture_output=True, check=True, timeout=10)
                print("‚úÖ Docker detected")
                runtime_available = True
            except (subprocess.CalledProcessError, FileNotFoundError, subprocess.TimeoutExpired):
                pass
        
        if not runtime_available:
            print("‚ö†Ô∏è No container runtime (Podman/Docker) available, skipping container tests")
            return True
        
        # Check if virtual environment exists and use it
        venv_python = project_root / ".venv" / "bin" / "python"
        if venv_python.exists():
            python_executable = str(venv_python)
            print("‚úÖ Using virtual environment Python")
        else:
            python_executable = sys.executable
            print("‚ö†Ô∏è Using system Python")
        
        # Run container-specific tests
        result = subprocess.run([
            python_executable, '-m', 'pytest', 
            'tests/test_podman_containers.py', 
            'tests/test_podman_integration.py',
            '-v', '--tb=short'
        ], capture_output=False, cwd=project_root)
        
        if result.returncode == 0:
            print("\n‚úÖ Container tests passed!")
            return True
        else:
            print("\n‚ö†Ô∏è Some container tests had issues")
            return False
            
    except Exception as e:
        print(f"\n‚ùå Error running container tests: {e}")
        return False


def main():
    """Main test runner function"""
    parser = argparse.ArgumentParser(description="Red Hat Status Checker Test Runner")
    parser.add_argument('--module', help='Run tests for specific module')
    parser.add_argument('--flags', action='store_true', help='Test all CLI flags')
    parser.add_argument('--examples', action='store_true', help='Test example commands from documentation')
    parser.add_argument('--advanced', action='store_true', help='Test advanced command combinations')
    parser.add_argument('--containers', action='store_true', help='Test container functionality (Podman/Docker)')
    parser.add_argument('--install-deps', action='store_true', help='Install test dependencies')
    parser.add_argument('--all', action='store_true', help='Run all tests (default)')
    
    args = parser.parse_args()
    
    if args.install_deps:
        return install_test_dependencies()
    
    if args.module:
        return run_specific_test_module(args.module)
    
    if args.flags:
        return run_flag_tests()
    
    if args.examples:
        return run_example_commands_test()
    
    if args.advanced:
        return run_advanced_commands_test()
    
    if args.containers:
        return 0 if run_container_tests() else 1
    
    if args.all or not any([args.module, args.flags, args.examples, args.advanced, args.containers]):
        # Run comprehensive test suite
        print("Running comprehensive test suite...")
        
        # First run unit tests with coverage
        unit_result = run_pytest_with_coverage()
        
        # Then run flag tests
        flag_result = run_flag_tests()
        
        # Then run example commands
        example_result = run_example_commands_test()
        
        # Then run advanced commands
        advanced_result = run_advanced_commands_test()
        
        # Then run container tests
        container_result = 0 if run_container_tests() else 1
        
        # Summary
        print("\n" + "=" * 60)
        print("COMPREHENSIVE TEST SUMMARY")
        print("=" * 60)
        print(f"Unit Tests: {'PASSED' if unit_result == 0 else 'FAILED'}")
        print(f"Flag Tests: {'PASSED' if flag_result == 0 else 'FAILED'}")
        print(f"Example Commands: {'PASSED' if example_result == 0 else 'FAILED'}")
        print(f"Advanced Commands: {'PASSED' if advanced_result == 0 else 'FAILED'}")
        print(f"Container Tests: {'PASSED' if container_result == 0 else 'FAILED'}")
        
        overall_result = 0 if all(r == 0 for r in [unit_result, flag_result, example_result, advanced_result, container_result]) else 1
        print(f"Overall: {'PASSED' if overall_result == 0 else 'FAILED'}")
        
        return overall_result


if __name__ == '__main__':
    sys.exit(main())
